%! TEX root = **/010-main.tex
% vim: spell spelllang=en:

\section{Description of pre-processing of data}%
\label{sec:desc-prep}

% Kind of preprocessing done to the original data:  Have you simplified the dataset?
% Removed some examples?
% Feature selection done?
% Did you enrich your dataset with other columns or more information?
% Imputing/removing missing values? 
% Simplification of values? 
% Normalization? 
% Remember that you should describe the all procedures performed on your raw dataset.

\subsection{Feature Removal}

Initially, we removed all of the variables which didn't provide any useful information
such as names and identification numbers: \texttt{kepid}, \texttt{rowid}, 
\texttt{kepoi\_name}, \texttt{kepler\_name} or \texttt{koi\_tce\_delivname}.

Next, we investigated the presence of missing data in the dataset, and we found that three
variables were mostly missing. Specially \texttt{koi\_teq\_err1} and 
\texttt{koi\_teq\_err2}, which represent the positive and negative error 
of \texttt{koi\_teq} respectively, are completely missing. Therefore, we proceeded to 
remove them as they didn't provide any information whatsoever. How we treated the rest
of the missing values will be explained in an upcoming section.

We also found four features that were flags representing whether the exoplanet
measurements had passed some tests. Since this flags directly impact the
target feature, as \texttt{koi\_disposition} will be FALSE NEGATIVE when it fails
at least one of the tests and CONFIRMED when it passes all the tests and is confirmed,
and are calculated using the raw data thus simplifying the problem, we decided to 
remove them because they described the target very accurately. Furthermore, the
main objective of this classification problem is to work directly with the raw Kepler
measurements and not with processed data such as these flags.

Furthermore, there are two other columns, \texttt{koi\_pdisposition} and 
\texttt{koi\_score},
that we decided to erase as well, as 
\texttt{koi\_pdisposition} represents the predicted \texttt{koi\_disposition}
(our target) and \texttt{koi\_score} illustrates the level of confidence of
that prediction, based on the aforementioned flags. Hence, they are directly
linked to our target as well. As a matter of fact, in close to 70\% of the
7316 remaining examples the \texttt{koi\_disposition} and \texttt{koi\_pdisposition}
have the same value.

\subsection{Example Removal}

As explained in the description of the original dataset, we decided to remove all 
examples that had \texttt{koi\_dispoistion} equal to CANDIDATE, as we want to
train and validate our models using exoplanets that we know for sure whether
they are or aren't confirmed, and not with examples that are pending for validation.
This resulted in the elimination of 2248 entries out of the original 9564, 
leaving us with 7316 examples.

\subsection{Value Transformation}

Because some algorithms only work with numerical features, we had to transform our
target column from categorical to numerical. Since we had removed all instances of
CANDIDATE, there were no examples of NOT DISPOSITIONED and the categories don't follow
a particular order, we opted for One-Hot encoding. Furthermore, as we only had two
categories, we didn't need to create extra columns, we simply transformed the existing
\texttt{koi\_disposition} column to be 1 when the example is CONFIRMED and 0 when
it isn't, meaning it is a FALSE POSITIVE.

\subsection{Error Treatment}

We scanned our dataset looking for errors and we weren't able to find any. We checked
that numerical values were within the plausible ranges, like for example that the
planet radius or temperatures were positive. We also verified that all categorical 
features only contained the expected categories.

\subsection{Missing Data Treatment}

For the remaining missing data, we decided to fill those empty cells with the mean
of the feature. We were able to do that because all of our columns with missing values
were of numerical type.

\subsection{Feature Selection}%
\label{sub:feature_removal}

The last step in our pre-processing 


\begin{figure}[H]
    \centering
    \includegraphics{kbest}
    \caption{Cross validation score for different $k$ values}%
    \label{fig:feature_cross}
\end{figure}

\begin{table}[H]
    \centering
    \caption{Selected features (25)}%
    \label{tab:features}
    \input{../tables/feature-score-25-nice}
\end{table}