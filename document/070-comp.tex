%! TEX root = **/010-main.tex
% vim: spell spelllang=en:

\section{Comparison}%
\label{sec:comparison}

% Comparison and discussion of results of the different data mining methods on the validation data-set.
% Which is the best method when testing on the validation data set?  Write a comparative table.
% Try to compare different methods using McNemar test or at least show the interval of confidence for each method.
% Is there an explanation for those results (some hypothesis applicable,  etc.)? 
% Are in general accuracy on validation data set similar to the obtained with cross-validation?   
% Are  there  for  some  methods  huge  differences? If  thatâ€™s  the  case,why do you think that happens?
% Final personal evaluation of which is the best method you consider and why.

Once we have all the algorithms implemented we are going to compare them to find out which yields the best results in our dataset. To do so we run the algorithms with their respective best parameters on the testing dataset and we get the following table:

\begin{table}[H]
\centering
\caption{Comparison of metrics}%
\label{tab:comparison}
\input{../tables/metrics}
\end{table}

Looking at \cref{tab:comparison} we can see that meta learning algorithms have the best f1-scores and accuracies. Bagging 

\begin{figure}[H]
\centering
\includegraphics{mcnemar}
\caption{mcNemar statistic}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics{mcnemar_pvalue}
\caption{mcNemar test $p$-value}
\end{figure}

\begin{table}[H]
\centering
\caption{mcNemar test $p$-values}
    \input{../tables/mcnemar_pvalues}
\end{table}