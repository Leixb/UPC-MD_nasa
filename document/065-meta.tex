%! TEX root = **/010-main.tex
% vim: spell spelllang=en:

\subsection{Meta-learning algorithms}%
\label{sub:meta}

\subsubsection{Performance Majority Voting}

Majority voting uses several of the algorithms already commented in this project. Therefore we will use those algorithms with the best combination of preprocessing and hyperparameters we already found.

\begin{table}[H]
\centering
\caption{Majority voting results}
\begin{tabular}{lr}
\toprule
Method & Accuracy \\
\midrule
Na\"ive Bayes & 0.884 \\
K-NN & 0.857 \\
Decision Tree & 0.877 \\
\addlinespace[0.5em]
Majority voting & 0.914 \\
Majority voting (weighted)  & 0.914 \\
\bottomrule
\end{tabular}
\end{table}


With hard voting:
\sresults{ 375 &  33 \\ 20 & 172 }{ 0.9117 }

\noindent
With weighted voting (2 1 2):
\sresults{ 373 &  35 \\ 19 & 173 }{ 0.9100 }

There is no benefit to weighting the votes since all 3 methods offer very similar accuracy.
 
\pagebreak
\subsubsection{Bagging}

\begin{figure}[H]
\centering
\includegraphics{bagging}
\caption{Bagging parameter search}%
\label{fig:bagging}
\end{figure}

The best results where obtained with: $\texttt{n\_est} = 200, \texttt{max\_features} = 0.9$. Nonetheless, as we
can see in \cref{fig:bagging} results with $\texttt{n\_est} \geq 50, \texttt{max\_features} \geq 0.3$ are pretty
similar.

\sresults{ 388 &  20 \\ 24  & 168 }{0.9267}

\begin{verbatim}
\end{verbatim}
 
\pagebreak
\subsubsection{RandomForest}

Random Forest is an algorithm based in the combination of multiple decision trees. It has many hyperparameters among which we decided to optimize the 6 following: \texttt{bootstrap}, \texttt{max\_features}, \texttt{max\_features}, \texttt{min\_samples\_leaf}, \texttt{min\_samples\_split}, \texttt{n\_estimators}. At the beginning we wanted to have a pool of 
many values per parameter. However the time of computation was too high and we ended up doing a 5-fold with 2-3 values per parameter. 

\begin{table}[H]
\centering
\caption{RandomForest best parameters}
\begin{tabular}{lr}
\toprule
Parameter & Value \\
\midrule
\texttt{bootstrap} & True \\
\texttt{max\_depth} & 150 \\
\texttt{max\_features} & 10 \\
\texttt{min\_samples\_leaf} & 5 \\
\texttt{min\_samples\_split} & 5 \\
\texttt{n\_estimators} & 500 \\
\bottomrule
\end{tabular}
\end{table}

\fresults{ 384 &  24 \\ 21 & 171 }{0.925}{0.883}

\subsubsection{Adaboost}

The final algorithm we implemented is Ada boosting. The base implementation of this algorithms works with decision trees as it's classifier. This type of boosting basically consists on performing different executions of the classifier but changing the weights. The algorithm has two additional hyperparameters \texttt{n\_estimators} and \texttt{learning\_rate}.

After that we fined tuned the parameters and we found that the best combination is \texttt{n\_estimators} = 60 and \texttt{learning\_rate} = 0.5

\begin{figure}[H]
\centering
\includegraphics{boosting}
\caption{AdaBoost parameter search}%
\label{fig:boosting}
\end{figure}

When executing Adaboost with the best parameters found we obtain the following results:

\fresults{ 383 &  25 \\ 21 & 171 }{0.923}{0.881}


% Performance  Majority  Voting,  Bagging, RandomForest  and  Adaboost.   Explain  parameters  selected  for  each  algorithm